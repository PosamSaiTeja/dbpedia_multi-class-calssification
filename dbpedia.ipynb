{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dbpedia dataset multiclass classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the data\n",
    "The DBpedia ontology classification dataset is constructed by picking 14 non-overlapping classes from DBpedia 2014. They are listed in classes.txt. From each of thse 14 ontology classes, we randomly choose 40,000 training samples and 5,000 testing samples. Therefore, the total size of the training dataset is 560,000 and testing dataset 70,000.\n",
    "\n",
    "The files train.csv and test.csv contain all the training samples as comma-sparated values. There are 3 columns in them, corresponding to class index (1 to 14), title and content. The title and content are escaped using double quotes (\"), and any internal double quote is escaped by 2 double quotes (\"\"). There are no new lines in title or content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes ,model_selection,preprocessing,svm,metrics,linear_model,neighbors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer ,CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "import xgboost as xg\n",
    "import string as strng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/msc2/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/msc2/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/msc2/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/msc2/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/msc2/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/msc2/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/msc2/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/msc2/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/msc2/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/msc2/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/msc2/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/msc2/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence,text\n",
    "from keras import layers, optimizers, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data1 = pd.read_csv('train.csv')\n",
    "test_data1 = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of         class                              title  \\\n",
       "0           1                   E. D. Abbott Ltd   \n",
       "1           1                     Schwan-Stabilo   \n",
       "2           1                         Q-workshop   \n",
       "3           1  Marvell Software Solutions Israel   \n",
       "4           1        Bergan Mercy Medical Center   \n",
       "...       ...                                ...   \n",
       "559995     14                   Barking in Essex   \n",
       "559996     14                   Science & Spirit   \n",
       "559997     14             The Blithedale Romance   \n",
       "559998     14                Razadarit Ayedawbon   \n",
       "559999     14           The Vinyl Cafe Notebooks   \n",
       "\n",
       "                                                  content  \n",
       "0        Abbott of Farnham E D Abbott Limited was a Br...  \n",
       "1        Schwan-STABILO is a German maker of pens for ...  \n",
       "2        Q-workshop is a Polish company located in Poz...  \n",
       "3        Marvell Software Solutions Israel known as RA...  \n",
       "4        Bergan Mercy Medical Center is a hospital loc...  \n",
       "...                                                   ...  \n",
       "559995   Barking in Essex is a Black comedy play direc...  \n",
       "559996   Science & Spirit is a discontinued American b...  \n",
       "559997   The Blithedale Romance (1852) is Nathaniel Ha...  \n",
       "559998   Razadarit Ayedawbon (Burmese: ရာဇာဓိရာဇ် အရေး...  \n",
       "559999   Vinyl Cafe Notebooks: a collection of essays ...  \n",
       "\n",
       "[560000 rows x 3 columns]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data1.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of        class                     title  \\\n",
       "0          1                     TY KU   \n",
       "1          1     Odd Lot Entertainment   \n",
       "2          1                    Henkel   \n",
       "3          1                GOAT Store   \n",
       "4          1  RagWing Aircraft Designs   \n",
       "...      ...                       ...   \n",
       "69995     14            Energy Victory   \n",
       "69996     14                 Bestiario   \n",
       "69997     14         Wuthering Heights   \n",
       "69998     14             L'Indépendant   \n",
       "69999     14      The Prophecy (novel)   \n",
       "\n",
       "                                                 content  \n",
       "0       TY KU /taɪkuː/ is an American alcoholic bever...  \n",
       "1       OddLot Entertainment founded in 2001 by longt...  \n",
       "2       Henkel AG & Company KGaA operates worldwide w...  \n",
       "3       The GOAT Store (Games Of All Type Store) LLC ...  \n",
       "4       RagWing Aircraft Designs (also called the Rag...  \n",
       "...                                                  ...  \n",
       "69995   Energy Victory: Winning the War on Terror by ...  \n",
       "69996   Bestiario is a book of 8 short stories writte...  \n",
       "69997   Wuthering Heights is a novel by Emily Brontë ...  \n",
       "69998   L'Indépendant is a newspaper published in Lux...  \n",
       "69999   The Prophecy is the fifth novel by New York T...  \n",
       "\n",
       "[70000 rows x 3 columns]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data1.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking whether the data represents all the classes equally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "1     40000\n",
       "2     40000\n",
       "3     40000\n",
       "4     40000\n",
       "5     40000\n",
       "6     40000\n",
       "7     40000\n",
       "8     40000\n",
       "9     40000\n",
       "10    40000\n",
       "11    40000\n",
       "12    40000\n",
       "13    40000\n",
       "14    40000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_by_class = train_data1.groupby(['class'])\n",
    "group_by_class.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "1     5000\n",
       "2     5000\n",
       "3     5000\n",
       "4     5000\n",
       "5     5000\n",
       "6     5000\n",
       "7     5000\n",
       "8     5000\n",
       "9     5000\n",
       "10    5000\n",
       "11    5000\n",
       "12    5000\n",
       "13    5000\n",
       "14    5000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_by_class = test_data1.groupby(['class'])\n",
    "group_by_class.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking subset of train data and test data to work with\n",
    "It is found with experimentation that the time taken to run the models considering the full fledged data is quite high and sometimes , the kernel dies. Inorder to overcome this problem, we could take a subset of data from the original data.If we take first few records  in sequence , we might end up getting the data representing only few classes. So, idea is to take a fraction of data from the original data randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of         class                                              title  \\\n",
       "549526     14                                    L'Arrêt de mort   \n",
       "331944      9                                 Anjir Siah-e Sofla   \n",
       "64819       2                          North Garland High School   \n",
       "198353      5                                       Trần Văn Tuý   \n",
       "362141     10                                         Peropteryx   \n",
       "...       ...                                                ...   \n",
       "109645      3                              Francis George Fowler   \n",
       "430730     11                                      Persea campii   \n",
       "557761     14  Holy Hell: A Memoir of Faith Devotion and Pure...   \n",
       "113031      3                                     Alexander Hall   \n",
       "78612       2  Central Institute of Plastics Engineering & Te...   \n",
       "\n",
       "                                                  content  \n",
       "549526   Death Sentence (French: L'Arrêt de mort) is a...  \n",
       "331944   Anjir Siah-e Sofla (Persian: انجيرسياه سفلي‎ ...  \n",
       "64819    North Garland High School is a public seconda...  \n",
       "198353   Trần Văn Tuý (born 20 July 1957 in Bắc Ninh P...  \n",
       "362141   Peropteryx is a genus of 5 species of bat in ...  \n",
       "...                                                   ...  \n",
       "109645   Francis George Fowler (1871–1918) familiarly ...  \n",
       "430730   Persea campii is a species of plant in the La...  \n",
       "557761   Holy Hell: A Memoir of Faith Devotion and Pur...  \n",
       "113031   Alexander Hall (January 11 1894 Boston Massac...  \n",
       "78612    The Central Institute of Plastics Engineering...  \n",
       "\n",
       "[84000 rows x 3 columns]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The data huge and processing is taking time so we shall take randomly some 15% of test data and\n",
    "#15 % of train data and work on that data only\n",
    "\n",
    "train_data = train_data1.sample(frac=0.15) # from the whole data, we are taking only 15/100 th fraction of data\n",
    "train_data.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "1     5979\n",
       "2     5942\n",
       "3     5924\n",
       "4     5944\n",
       "5     6202\n",
       "6     5805\n",
       "7     6047\n",
       "8     5824\n",
       "9     6024\n",
       "10    6020\n",
       "11    6001\n",
       "12    6019\n",
       "13    6126\n",
       "14    6143\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_by_class = train_data.groupby(['class'])\n",
    "group_by_class.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of        class                          title  \\\n",
       "15434      4                    Heiner Dopp   \n",
       "18577      4                Shannon Welcome   \n",
       "51716     11        Andropogon benthamianus   \n",
       "67367     14               A Spot of Bother   \n",
       "1240       1  Maianbar Bundeena Bus Service   \n",
       "...      ...                            ...   \n",
       "26149      6             USS Nausett (1865)   \n",
       "25475      6   Metropolitan Railway F Class   \n",
       "41498      9                         Kožuhe   \n",
       "43295      9              Firuzabad Mashhad   \n",
       "35793      8                  Kasilof River   \n",
       "\n",
       "                                                 content  \n",
       "15434   Heiner Dopp (born 27 June 1956 in Bad Dürkhei...  \n",
       "18577   Shannon Roy Welcome Warren (born 22 November ...  \n",
       "51716   Andropogon benthamianus is a species of grass...  \n",
       "67367   A Spot of Bother is the second adult novel by...  \n",
       "1240    Mainanbar Bundeena Bus Service is an Australi...  \n",
       "...                                                  ...  \n",
       "26149   USS Nausett a single-turreted twin-screw moni...  \n",
       "25475   The Metropolitan Railway F class was a class ...  \n",
       "41498   Kožuhe (Cyrillic: Кожухе) is a village in the...  \n",
       "43295   Firuzabad (Persian: فيروزاباد‎ also Romanized...  \n",
       "35793   The Kasilof River (/kəˈsiːlɒf/ kə-SEE-lof) or...  \n",
       "\n",
       "[10500 rows x 3 columns]>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_data1.sample(frac=0.15) # from the whole data, we are taking only 15/100 th fraction of data\n",
    "test_data.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "1     786\n",
       "2     768\n",
       "3     713\n",
       "4     758\n",
       "5     757\n",
       "6     739\n",
       "7     761\n",
       "8     758\n",
       "9     724\n",
       "10    735\n",
       "11    766\n",
       "12    712\n",
       "13    762\n",
       "14    761\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_by_class = test_data.groupby(['class'])\n",
    "group_by_class.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_data =train_data[['content']]\n",
    "y_train_data = train_data[['class']]\n",
    "x_test_data = test_data[['content']]\n",
    "y_test_data = test_data[['class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_list = x_train_data['content'].tolist()\n",
    "y_train_list = y_train_data['class'].tolist()\n",
    "x_test_list = x_test_data['content'].tolist()\n",
    "y_test_list = y_test_data['class'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing \n",
    "* we are using the if-idf vectorrizer\n",
    "* we are taking one word at a time while doing tf-idf\n",
    "* we are removiing the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000,stop_words='english')\n",
    "tfidf_vect.fit(x_train_list)\n",
    "xtrain_tfidf =  tfidf_vect.transform(x_train_list)\n",
    "xvalid_tfidf =  tfidf_vect.transform(x_test_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume the independence in case of Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9444761904761905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[682,   6,   5,   0,   3,   8,  16,   0,   0,   0,   6,   0,   0,\n",
       "         12],\n",
       "       [  7, 746,   1,   0,   3,   1,  32,   0,   4,   0,   0,   0,   0,\n",
       "          0],\n",
       "       [  5,   1, 585,   5,  12,   1,   2,   0,   1,   3,   0,   2,   2,\n",
       "          7],\n",
       "       [  3,   3,   3, 748,   3,   1,   0,   0,   0,   9,   0,   0,   0,\n",
       "          4],\n",
       "       [  2,   4,  14,   3, 727,   0,   2,   0,   0,   3,   1,   0,   0,\n",
       "          3],\n",
       "       [ 17,   2,   0,   0,   3, 725,   1,   1,   1,   1,   0,   0,   0,\n",
       "          1],\n",
       "       [ 14,   5,   1,   0,   2,   1, 697,   6,   6,   0,   1,   0,   0,\n",
       "          1],\n",
       "       [  1,   0,   0,   0,   0,   1,   7, 750,  11,   3,   0,   0,   0,\n",
       "          0],\n",
       "       [  0,   1,   0,   0,   0,   0,   0,   0, 700,   0,   0,   0,   0,\n",
       "          0],\n",
       "       [  0,   0,   0,   0,   0,   0,   1,   0,   1, 659,  18,   0,   0,\n",
       "          3],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  57, 739,   0,   0,\n",
       "          0],\n",
       "       [ 27,   0,  41,   0,   0,   0,   0,   0,   0,   0,   0, 707,   6,\n",
       "          1],\n",
       "       [  8,   0,  12,   0,   2,   0,   1,   0,   0,   0,   0,   3, 745,\n",
       "         22],\n",
       "       [ 20,   0,  51,   2,   2,   1,   2,   1,   0,   0,   1,   0,   9,\n",
       "        707]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mMultinomial Naive bayesian model \n",
    "x=naive_bayes.MultinomialNB().fit(xtrain_tfidf,y_train_list)\n",
    "predictions = x.predict(xvalid_tfidf)\n",
    "accuracy=metrics.accuracy_score(predictions, y_test_list)\n",
    "print(accuracy)\n",
    "confusion_matrix=metrics.confusion_matrix(predictions, y_test_list)\n",
    "confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9439932789827418\n",
      "0.9444761904761906\n",
      "0.9449425861774863\n"
     ]
    }
   ],
   "source": [
    "print (metrics.f1_score(predictions, y_test_list,average='macro'))\n",
    "print (metrics.f1_score(predictions, y_test_list,average='micro'))\n",
    "print (metrics.f1_score(predictions, y_test_list,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy with Logistic regression model is 0.9709523809523809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msc2/.local/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression \n",
    "\n",
    "x=linear_model.LogisticRegression().fit(xtrain_tfidf,y_train_list)\n",
    "predictions = x.predict(xvalid_tfidf)\n",
    "predictions_proba = x.predict_proba(xvalid_tfidf)\n",
    "\n",
    "accuracy=metrics.accuracy_score(predictions, y_test_list)\n",
    "print(\"the accuracy with Logistic regression model is\",accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score for logistic regression with macro average 0.9710350443802884\n",
      "F1-score for logistic regression with micro average 0.9709523809523809\n",
      "F1-score for logistic regression with weighted average 0.9709543337491622\n"
     ]
    }
   ],
   "source": [
    "print(\"F1-score for logistic regression with macro average\",metrics.f1_score(y_test_list, predictions, average='macro'))\n",
    "print(\"F1-score for logistic regression with micro average\",metrics.f1_score(y_test_list, predictions, average='micro'))\n",
    "print(\"F1-score for logistic regression with weighted average\",metrics.f1_score(y_test_list, predictions, average='weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42933333333333334"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KNN \n",
    "\n",
    "x=neighbors.KNeighborsClassifier().fit(xtrain_tfidf,y_train_list)\n",
    "predictions3 = x.predict(xvalid_tfidf)\n",
    "accuracy=metrics.accuracy_score(predictions3, y_test_list)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score for KNN with macro average 0.48188868562474907\n",
      "F1-score for KNN with micro average 0.4293333333333333\n",
      "F1-score for KNN with weighted average 0.4820763641736975\n"
     ]
    }
   ],
   "source": [
    "print(\"F1-score for KNN with macro average\",metrics.f1_score(y_test_list, predictions3, average='macro'))\n",
    "print(\"F1-score for KNN with micro average\",metrics.f1_score(y_test_list, predictions3, average='micro'))\n",
    "print(\"F1-score for KNN with weighted average\",metrics.f1_score(y_test_list, predictions3, average='weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9722857142857143"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SVM\n",
    "x=svm.SVC().fit(xtrain_tfidf,y_train_list)\n",
    "predictions4 = x.predict(xvalid_tfidf)\n",
    "accuracy=metrics.accuracy_score(predictions4, y_test_list)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score for SVM with macro average 0.9723869933922656\n",
      "F1-score for SVM with micro average 0.9722857142857143\n",
      "F1-score for SVM with weighted average 0.9722981188738296\n"
     ]
    }
   ],
   "source": [
    "print(\"F1-score for SVM with macro average\",metrics.f1_score(y_test_list, predictions4, average='macro'))\n",
    "print(\"F1-score for SVM with micro average\",metrics.f1_score(y_test_list, predictions4, average='micro'))\n",
    "print(\"F1-score for SVM with weighted average\",metrics.f1_score(y_test_list, predictions4, average='weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "x=ensemble.RandomForestClassifier().fit(xtrain_tfidf,y_train_list)\n",
    "predictions5 = x.predict(xvalid_tfidf)\n",
    "accuracy=metrics.accuracy_score(predictions5, y_test_list)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score for Random_forest with macro average 0.9498809150604893\n",
      "F1-score for Random_forest with micro average 0.9500000000000001\n",
      "F1-score for Radom_forest with weighted average 0.949819327115977\n"
     ]
    }
   ],
   "source": [
    "print(\"F1-score for Random_forest with macro average\",metrics.f1_score(y_test_list, predictions5, average='macro'))\n",
    "print(\"F1-score for Random_forest with micro average\",metrics.f1_score(y_test_list, predictions5, average='micro'))\n",
    "print(\"F1-score for Radom_forest with weighted average\",metrics.f1_score(y_test_list, predictions5, average='weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9305714285714286"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XG Boost classifier\n",
    "x=xg.XGBClassifier().fit(xtrain_tfidf,y_train_list)\n",
    "predictions6 = x.predict(xvalid_tfidf)\n",
    "accuracy=metrics.accuracy_score(predictions6, y_test_list)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score for Xgboost with macro average 0.9309263178654514\n",
      "F1-score for Xgboost  with micro average 0.9305714285714286\n",
      "F1-score for Xgboost with weighted average 0.9307439320986206\n"
     ]
    }
   ],
   "source": [
    "print(\"F1-score for Xgboost with macro average\",metrics.f1_score(y_test_list, predictions6, average='macro'))\n",
    "print(\"F1-score for Xgboost  with micro average\",metrics.f1_score(y_test_list, predictions6, average='micro'))\n",
    "print(\"F1-score for Xgboost with weighted average\",metrics.f1_score(y_test_list, predictions6, average='weighted'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "* The data consists of 560,000 test data observations and 70,000 train data observations.\n",
    "* Each observation from both the test data and the train data belongs to one of the 14 classes.\n",
    "* Checked for the balace of data \n",
    "* Found out that working with whole data is taking long time and sometimes system is hanging.\n",
    "* So, took a random sample which contains 15/100 fraction of original data(train data =84,000 observations,test data=10,500 observations)\n",
    "* After taking sample also, checked whether the data is balanced or not\n",
    "* Did pre-processing steps like tf-idf tokenization and removing stop-words\n",
    "* Then considered baseline model as Naive Bayes model\n",
    "* Different models and their accuracies and F1-scores:\n",
    "\n",
    "\n",
    "| Model | Accuracy| F1-Score|\n",
    "| ---------------------- | ---------------------- | ---------------------- |\n",
    "| Naive-Bayesian | 94.4 | 94.49(Macro Average) |\n",
    "| LogisticRegression | 97.09 | 97.01(Macro Average) |\n",
    "| KNeighborsClassifier | 42.9 | 48.18(Macro Average) |\n",
    "| __Linear SVM__ | __97.2__ | __97.3(Macro Average)__ |\n",
    "| RandomForestClassifier | 95| 95(Macro Average) |\n",
    "| XGBClassifier | 93.05 | 93.09(Macro Average) |\n",
    "\n",
    "\n",
    "* __Conclusion__:\n",
    "\n",
    "    Linear SVM and Logistic Regression models give the highest accuracy as well as F1-score ie.,97.2 and 97.02         respectively. With the assumed data , SVM seems to be the best model. However, accuracies might change when         whole data is considered."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,auto:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
